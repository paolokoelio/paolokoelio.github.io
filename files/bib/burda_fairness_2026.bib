@inproceedings{burda_fairness_2026,
	title = {Fairness trade-offs in hiring: what people prefer and what engineers can build},
	abstract = {Human-centered AI must confront tensions between mutually incompatible fairness definitions and fairness requirements of algorithmic decision-making (ADM) systems. To investigate how people perceive this trade-off and how this perception can guide engineering requirements, we determine the underlying principles of common fairness metrics in the form of statements that people may or may not agree with. Using an illustrative dataset, we show how favored metrics can conflict in practice, underscoring the need for explicit trade-offs and how to solve them. We design and evaluate a survey that can be used to determine the preferences of stakeholders in a hiring scenario by mapping 12 statements to demographic parity, equal opportunity (TPR), predictive equality (FPR), predictive parity (PPV), fairness through unawareness, and individual fairness definitions. Responses (N=51) indicate broad support for excluding sensitive attributes and for error-rate parity criteria (FPR-TPR), with contrasting views on demographic parity under unequal base rates. We contribute a requirements-elicitation approach that can be used to define ‘fairness requirements’ of an ADM system by mapping stakeholder preferences to concrete metrics, yielding a pragmatic set of recommended requirements using our hiring scenario as a guiding example.},
	language = {en},
	booktitle = {Human {Centred} {Artificial} {Intelligence} - {Education} and {Practice} ({HCAI}-{EP} ’26),},
	publisher = {ACM},
	author = {Burda, Pavlo and van Otterloo, Sieuwert},
	year = {2026},
	file = {PDF:C\:\\Users\\p_ict\\Zotero\\storage\\3UPT5EGS\\Burda and van Otterloo - 2026 - Fairness trade-offs in hiring what people prefer and what engineers can build.pdf:application/pdf},
}
